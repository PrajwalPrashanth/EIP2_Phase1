{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documented_9.4L_DNST_CIFAR10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "LbBu0Of8Rp2S",
        "bE47RsDJXsBJ",
        "vw8V-XiaScM2",
        "Z3KpmoQ3Vk_d",
        "2i-hP15TXEdb",
        "i_E0zyhXX1Xp",
        "mDCOxqxYSU4i"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrajwalPrashanth/EIP2_Phase1/blob/master/Session%204/Documented_9_4L_DNST_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Jd0TRAAZ6_rG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Genral Summary \n",
        "\n",
        "* Used Google Colab for GPU \n",
        "* Ran with 5 different Accounts to (4 windows in one virtual desktop to test different models and 1 for experimentation of ideas that pops or something written in a blog)\n",
        "* Some of the models and logs were lost when the browser crashed and output cells were gone, i'll add the links below for other model which did'nt go wrong\n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/file/d/1mIgtVtBouHEZub9kMNJo2TrwFsqUAvjt/view?usp=sharing)"
      ]
    },
    {
      "metadata": {
        "id": "gQvBefFO8itq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Summary \n",
        "\n",
        "* This is the folow-up of my first submission which was done on Oct 22nd 11:53pm (5 days after my 4th session) had then validation accuracy 92.8% and moved upto 93.41% now.\n",
        "\n",
        "* This model has total params of 940k and it's using compression only not bottleneck\n",
        "\n",
        "* For first 50 epochs i have twice the steps per epoch as i had read in the keras doc the generator will keep on generating the images, so i made it twice the no. of images , as i read in a medium post that will help , tried thrice didnt help much and it took a lot of time . as im not sure how this will be graded i have done other model with 50k only i.e same no. of steps for as much as 50k images.\n",
        "\n",
        "**Model**\n",
        "\n",
        "* Went through blogs to understand about densenet as i was not getting much out of the paper, after some understanding gotto know about the terminologies and hit the paper once again.\n",
        "* The filters for each layer in dense block were too low in the default code u had shared as it was multiplied with compression so went ahead and replaced with a separate variable as growth_rate\n",
        "* In some Models i have implemented the actual bottleneck like mentioned in the paper\n",
        "* Tried different variations out of it and did experimenations.\n",
        "* Using only 3 pairs of dense and transistion block\n",
        "\n",
        "**Batch Size**\n",
        "\n",
        "* Ranged from 32 to 256. In most cases 64 was the best.\n",
        "\n",
        "**Learning Rate**\n",
        "\n",
        "* This was the key tuning parameter to reach the mark\n",
        "* Was using Cyclic Learning Rate\n",
        "* Range 0.1 to 0.2 in the beginning for faster convergence\n",
        "* Then kept dividing by 10 after some epochs\n",
        "* In Some places i have used 0.075 to 0.15 based on graph on cifar-10 lr vs loss graph which was shown in CLR github repo.\n",
        "* Also in some places at the end have used static learning rate.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1Wck3JjrB5kO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models Summary \n",
        "\n",
        "\n",
        "\n",
        "1.   This Submission | val_acc: 93.41 | 109 epochs | 940k total params | twice steps per epoch | Densenet compression only\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2.   Same type like the submitted file Compression only model | val_acc 93.43 | too shaby so didnt submit this one | https://colab.research.google.com/drive/1-ab24n43TGBEEGxig6Rj6-3qP4BeqXPR\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3.  Replica of Densenet BC-100-12 | aprrox 793k total params | val_acc:93.24 |94  epochs | didnt use droput from the start | https://colab.research.google.com/drive/1ib3PGTNW4SlPGVqY7bHf0xhrQ6OvmCRs\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "4.  Like Densenet bottleneck compression but with more layers inside denseblock | 963k | val_acc:93.00 | 121 epochs | didnt drop augmentation completely | https://colab.research.google.com/drive/1RaI3DO_V74PFkKFJOl4EiuFOkP3Rvyf3\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "5. Tried wide densenet too though theri training time were almost half of this didnt find good accuracy , and did'nt tune it further.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LbBu0Of8Rp2S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initial imports and cifar-10 data"
      ]
    },
    {
      "metadata": {
        "id": "K70hAckqg0EA",
        "colab_type": "code",
        "outputId": "8b3434e5-73cf-4a80-a0ef-ef67d82930d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "wVIx_KIigxPV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import *\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UNHw6luQg3gc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mB7o3zu1g6eT",
        "colab_type": "code",
        "outputId": "5aa93983-fd2d-4d0e-8f28-802dae88a78c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bE47RsDJXsBJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Connecting to drive"
      ]
    },
    {
      "metadata": {
        "id": "9d-pfDmmXyDj",
        "colab_type": "code",
        "outputId": "7aacc97b-ec72-4022-88ef-9b0289f8fabd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Zmn5cfBiq_H",
        "colab_type": "code",
        "outputId": "b8de2a96-4450-417b-80d6-786d8ea5e33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat /content/gdrive/My\\ Drive/foo.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Google Drive!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vw8V-XiaScM2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Image Augmentation\n",
        "\n",
        "* Geometric transformations for the model to learn better on object's features than all over the image.\n",
        "* Zoom and shifts to make it like it has been cropped"
      ]
    },
    {
      "metadata": {
        "id": "XUY7sn4YSjeU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "                horizontal_flip = True,\n",
        "    width_shift_range = 0.1,height_shift_range = 0.1,zoom_range=0.1                                  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z3KpmoQ3Vk_d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Checkpoint\n",
        "\n",
        "To get along with colab to save weights when they improve\n"
      ]
    },
    {
      "metadata": {
        "id": "yPGbxeQ4Vndq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath=\"/content/gdrive/My Drive/test_9.4L/ep:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2i-hP15TXEdb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cyclic learning rates\n",
        "\n",
        "* Using Cyclic learning rates to converge faster in the begining and also not to get stuck in local minima which can happen using a static learning rate."
      ]
    },
    {
      "metadata": {
        "id": "k8WPFpn_XQUo",
        "colab_type": "code",
        "outputId": "5c26dd51-eef9-41a6-8966-95226cc01758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf clr_callback.py*\n",
        "!wget https://raw.githubusercontent.com/bckenstler/CLR/master/clr_callback.py\n",
        "from clr_callback import *"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-01 03:37:29--  https://raw.githubusercontent.com/bckenstler/CLR/master/clr_callback.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5326 (5.2K) [text/plain]\n",
            "Saving to: ‘clr_callback.py’\n",
            "\n",
            "\rclr_callback.py       0%[                    ]       0  --.-KB/s               \rclr_callback.py     100%[===================>]   5.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-11-01 03:37:29 (42.5 MB/s) - ‘clr_callback.py’ saved [5326/5326]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i_E0zyhXX1Xp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model block Fucntions\n",
        "* i got confused in the begining as the default code had used terminolgies like bottleneck, growth_rate differently as i was comparing with densenet paper initially.\n",
        "\n",
        "* got a blog which made it simpler for me to understand those terminolgies better\n",
        "\n",
        "* in default code num_filter * compression was acting like growth_rate went ahead and changed that to a separate variable to have more growth rate, which for me made a sensible change as there were only 6 channels added in the default code\n",
        "\n",
        "* num_filter was now global which only affected the first conv layer\n",
        "\n",
        "* Replaced global average pooling instead of flatten\n",
        "\n",
        "* Actually this model does'nt have bottle neck to reduce parameteres in dense block , it only densenet like model with compression \n"
      ]
    },
    {
      "metadata": {
        "id": "ee-sge5Kg7vr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    global num_filter\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(growth_rate, (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        num_filter = num_filter + growth_rate\n",
        "        temp = concat\n",
        "        \n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOP6IPsGhBwb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_transition(input, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    global num_filter\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    num_filter = num_filter * compression\n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0RaKFpubhDIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    GAvgPooling = GlobalAveragePooling2D()(relu)\n",
        "    output = Dense(num_classes, activation='softmax', use_bias=False)(GAvgPooling)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDCOxqxYSU4i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameters to build the model\n",
        "\n",
        "* introduced growth_rate as 12\n",
        "* made l(no. of layers of bn-relu-conv in one denseblock) as 15 as it was crossing the constraint with l=16 of 1M parameters \n",
        "* num_filter as 24 i felt 12 was pretty low, also for densenet bc 100 12 made use of 24 for first conv, made me make it 24"
      ]
    },
    {
      "metadata": {
        "id": "dsO_yGxcg5D8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_classes = 10\n",
        "l = 15\n",
        "num_filter = 24\n",
        "compression = 0.5\n",
        "dropout_rate = 0\n",
        "growth_rate = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3VoTeTxQX_v5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "* Stuck on to 3 denseblocks, by removing other last block which was added in the default code\n",
        "* Ended with  \n",
        "             Total params: 939,576\n",
        "             Trainable params: 922,392\n",
        "              Non-trainable params: 17,184"
      ]
    },
    {
      "metadata": {
        "id": "anPCpQWhhGb7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model\n",
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, dropout_rate)\n",
        "\n",
        "output = output_layer(Third_Block)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kFh7pdxhNtT",
        "colab_type": "code",
        "outputId": "4ff01bb7-519a-4810-f583-17932a8ca2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7658
        }
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 24)   648         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 24)   96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 24)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 12)   2592        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 36)   0           conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 36)   144         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 36)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 12)   3888        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 48)   0           concatenate_1[0][0]              \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 48)   192         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 48)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 12)   5184        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 60)   0           concatenate_2[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 60)   240         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 60)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 12)   6480        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 72)   0           concatenate_3[0][0]              \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 72)   288         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 72)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 12)   7776        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 84)   0           concatenate_4[0][0]              \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 84)   336         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 84)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 12)   9072        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 96)   0           concatenate_5[0][0]              \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 96)   384         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 96)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 12)   10368       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 108)  0           concatenate_6[0][0]              \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 108)  432         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 108)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 12)   11664       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 120)  0           concatenate_7[0][0]              \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 120)  480         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 120)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 12)   12960       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 132)  0           concatenate_8[0][0]              \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 132)  528         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 132)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 12)   14256       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 144)  0           concatenate_9[0][0]              \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 144)  576         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 144)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 12)   15552       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 156)  0           concatenate_10[0][0]             \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 156)  624         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 156)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 12)   16848       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 168)  0           concatenate_11[0][0]             \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 168)  672         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 168)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 12)   18144       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 32, 32, 180)  0           concatenate_12[0][0]             \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 32, 32, 180)  720         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 32, 32, 180)  0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 12)   19440       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 32, 32, 192)  0           concatenate_13[0][0]             \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 32, 32, 192)  768         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 32, 32, 192)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 12)   20736       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 32, 32, 204)  0           concatenate_14[0][0]             \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 204)  816         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 32, 32, 204)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 102)  20808       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 102)  0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 102)  408         average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 102)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 12)   11016       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 114)  0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 114)  456         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 114)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 12)   12312       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 126)  0           concatenate_16[0][0]             \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 126)  504         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 126)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 12)   13608       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 138)  0           concatenate_17[0][0]             \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 138)  552         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 138)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 12)   14904       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 150)  0           concatenate_18[0][0]             \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 150)  600         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 150)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 12)   16200       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 162)  0           concatenate_19[0][0]             \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 162)  648         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 162)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 12)   17496       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 174)  0           concatenate_20[0][0]             \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 174)  696         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 174)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 12)   18792       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 186)  0           concatenate_21[0][0]             \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 186)  744         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 186)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 12)   20088       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 198)  0           concatenate_22[0][0]             \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 198)  792         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 198)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 12)   21384       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 210)  0           concatenate_23[0][0]             \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 210)  840         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 210)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 12)   22680       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 16, 16, 222)  0           concatenate_24[0][0]             \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 222)  888         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 222)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 12)   23976       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 16, 16, 234)  0           concatenate_25[0][0]             \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 234)  936         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 234)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 16, 16, 12)   25272       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 16, 16, 246)  0           concatenate_26[0][0]             \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 246)  984         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 246)  0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 16, 16, 12)   26568       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 16, 16, 258)  0           concatenate_27[0][0]             \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 258)  1032        concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 258)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 16, 16, 12)   27864       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 16, 16, 270)  0           concatenate_28[0][0]             \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 16, 16, 270)  1080        concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 16, 16, 270)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 16, 16, 12)   29160       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 16, 16, 282)  0           concatenate_29[0][0]             \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 16, 16, 282)  1128        concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 16, 16, 282)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 16, 16, 141)  39762       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 141)    0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 141)    564         average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 141)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 12)     15228       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 153)    0           average_pooling2d_2[0][0]        \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 153)    612         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 153)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 12)     16524       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 165)    0           concatenate_31[0][0]             \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 165)    660         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 165)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 12)     17820       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 177)    0           concatenate_32[0][0]             \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 177)    708         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 177)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 12)     19116       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 189)    0           concatenate_33[0][0]             \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 189)    756         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 189)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 12)     20412       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 201)    0           concatenate_34[0][0]             \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 201)    804         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 201)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 12)     21708       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 213)    0           concatenate_35[0][0]             \n",
            "                                                                 conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 213)    852         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 213)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 12)     23004       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 8, 8, 225)    0           concatenate_36[0][0]             \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 8, 8, 225)    900         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 8, 8, 225)    0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 8, 8, 12)     24300       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 8, 8, 237)    0           concatenate_37[0][0]             \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 8, 8, 237)    948         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 8, 237)    0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 8, 8, 12)     25596       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 8, 8, 249)    0           concatenate_38[0][0]             \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 8, 8, 249)    996         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 8, 8, 249)    0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 8, 8, 12)     26892       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 8, 8, 261)    0           concatenate_39[0][0]             \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 8, 8, 261)    1044        concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 8, 8, 261)    0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 8, 8, 12)     28188       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 8, 8, 273)    0           concatenate_40[0][0]             \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 8, 8, 273)    1092        concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 8, 8, 273)    0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 8, 8, 12)     29484       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 8, 8, 285)    0           concatenate_41[0][0]             \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 8, 8, 285)    1140        concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 8, 8, 285)    0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 8, 8, 12)     30780       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 8, 8, 297)    0           concatenate_42[0][0]             \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 8, 8, 297)    1188        concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 8, 8, 297)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 8, 8, 12)     32076       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 8, 8, 309)    0           concatenate_43[0][0]             \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8, 8, 309)    1236        concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 8, 309)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 8, 8, 12)     33372       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 8, 8, 321)    0           concatenate_44[0][0]             \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 321)    1284        concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 321)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 321)          0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           3210        global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 939,576\n",
            "Trainable params: 922,392\n",
            "Non-trainable params: 17,184\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qx1j723AIETp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Opening in a new tab logs were not present unless went to open in playground , opening in a private window displayed the logs. Still if the logs are not available download zip file to get ipynb file in this gist link\n",
        "\n",
        "https://gist.github.com/PrajwalPrashanth/2a813f1f345be3a1e0ecfd8952c06efa"
      ]
    },
    {
      "metadata": {
        "id": "rbyci-PzYDzu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "As i was saving best val_acc for checkpoints i have used and with colab crashing and me leaving it to run in the background it would have been disconnected so have mentinoed effective epochs and the actual run in the brackets so there's no confusion."
      ]
    },
    {
      "metadata": {
        "id": "tFBQ7RhbaY4d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Effective Training Epochs (1 --> 46) (_run (1 --> 50)ends at 46_)\n",
        "\n",
        "* **CLR** | **range(0.1,0.2)** | **step_size=2000** \n",
        "\n",
        "* Have used twice steps_per_epoch for first 50 epochs \n",
        "\n",
        "* Again with training batch size 64 was giving good results initially\n",
        "\n",
        "* **max val_acc 90.46 in 46th epoch**"
      ]
    },
    {
      "metadata": {
        "id": "b4XOsW3ahSkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model.load_weights('/content/gdrive/My Drive/6l_clr/file_name-77-0.84.hdf5')\n",
        "clr_t = CyclicLR(mode='triangular2',base_lr=0.1,max_lr=0.2,step_size=2000)\n",
        "callbacks_list.append(clr_t)\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=SGD(),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "crhGk7kEhXAz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    epochs=epochs,\n",
        "                    steps_per_epoch = len(x_train)/32,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r1SFaM3JmY5_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Effective Training Epochs (47 --> 50) (_Run (1 --> 19)ends at 4_ )\n",
        "\n",
        "* **CLR** | **range(0.0075,0.015)** | **step_size=1000**  \n",
        "\n",
        "* Batch size 64\n",
        "\n",
        "* **max val_acc 91.92 in 50th(here 4th) epoch**"
      ]
    },
    {
      "metadata": {
        "id": "Ns9Jqi-DBW9r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/My Drive/2times:ep:045-val_acc:0.905.hdf5')\n",
        "clr_t = CyclicLR(mode='triangular',base_lr=0.0075,max_lr=0.015,step_size=1000)\n",
        "callbacks_list.append(clr_t)\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=SGD(),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3DS1BbSsBxAt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    epochs=30,\n",
        "                    verbose=1,\n",
        "                    steps_per_epoch = len(x_train)/32,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hyq1q7Htdmh5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Effective Training Epochs (51 --> 60) (_Run (1 --> 10)ends at 10_ )\n",
        "\n",
        "* **CLR** | **range(0.00075,0.0015)** | **step_size=500**  \n",
        "\n",
        "* Batch size 32\n",
        "\n",
        "* **max val_acc 91.97 in 60th(here 10th) epoch**"
      ]
    },
    {
      "metadata": {
        "id": "WpRS7K_U5jQQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/My Drive/2times:ep:004-val_acc:0.919.hdf5')\n",
        "clr_t = CyclicLR(mode='triangular',base_lr=0.00075,max_lr=0.0015,step_size=500)\n",
        "callbacks_list.append(clr_t)\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=SGD(),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VZaMYz5p5irl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vKiJYhBZeCOy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Effective Training Epochs (61 --> 63) (_Run (1 --> 10)ends at 3_ )\n",
        "\n",
        "* **CLR** | **range(0.00075,0.0015)** | **step_size=500**  \n",
        "\n",
        "* Batch size 32\n",
        "\n",
        "* **max val_acc 92.24 in 63rd(here 3rd) epoch**"
      ]
    },
    {
      "metadata": {
        "id": "MCJXgeEhDhmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/My Drive/2times:ep:49+010-val_acc:0.920.hdf5')\n",
        "clr_t = CyclicLR(mode='triangular',base_lr=0.000075,max_lr=0.00015,step_size=500)\n",
        "callbacks_list.append(clr_t)\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=SGD(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q1dBOOJbDzf-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GrMjDAPogb0q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ** Dropped Augmentation from 64th epoch onwards **\n",
        "\n",
        "* Used model.fit() to drop augmentation as i was not concatenating the origianal dataset and was only passing the augmented images the model was missing on some features i think so i dropped it at the end and after breaching 92% mark.  "
      ]
    },
    {
      "metadata": {
        "id": "FrNlaCKte3Ej",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Effective Training Epochs (64 --> 73) (_Run (1 --> 10)ends at 9_ )\n",
        "\n",
        "* lr .0001  \n",
        "\n",
        "\n",
        "* Batch size 64\n",
        "\n",
        "* **max val_acc 92.77 in 73rd(here 9th) epoch**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gA50VPHKWDDq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/My Drive/2times:ep:59+003-val_acc:0.922.hdf5')\n",
        "#clr_t = CyclicLR(mode='triangular',base_lr=0.000075,max_lr=0.00015,step_size=500)\n",
        "#callbacks_list.append(clr_t)\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=SGD(.0001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bhnQLdPtWDD0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=64,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HQg_yLSrWCUu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BObAnaPBfUpQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Effective Training Epochs (74 --> 84) (_Run (1 --> 10)ends at 10_ )\n",
        "\n",
        "* lr .0001  \n",
        "\n",
        "* Batch size 128\n",
        "\n",
        "* **max val_acc 92.80 in 83rd(here 9th) epoch**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9PIU06bCkdgK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/My Drive/2times:ep:62+009-val_acc:0.928.hdf5')\n",
        "#clr_t = CyclicLR(mode='triangular',base_lr=0.000075,max_lr=0.00015,step_size=500)\n",
        "#callbacks_list.append(clr_t)\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=SGD(.0001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oas9V1BFkdgc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=128,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1xXZEbQQf8la",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Effective Training Epochs (85 --> 88) (_Run (1 --> 10)ends at 4_ )\n",
        "\n",
        "* weights carried from last run as there was no disconnection \n",
        "* lr .00001  \n",
        "\n",
        "\n",
        "* Batch size 256\n",
        "\n",
        "* **max val_acc 92.81 in 88nd(here 4th) epoch**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fgyTgHrZxHUs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model.load_weights('/content/gdrive/My Drive/2times:ep:62+009-val_acc:0.928.hdf5')\n",
        "#clr_t = CyclicLR(mode='triangular',base_lr=0.000075,max_lr=0.00015,step_size=500)\n",
        "#callbacks_list.append(clr_t)\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=SGD(.00001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z11rDdbgxHU-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=256,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mAxlkuLo5apn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Dropped dropout from 89th epoch**\n",
        "\n",
        "* Dropped dropout too at the end so that it can learn some specific features too , i used dropout in the begining so that the model wont overfit and dropping it only at the end.\n",
        "\n",
        "* Made droput value 0 and ran the model again ."
      ]
    },
    {
      "metadata": {
        "id": "31YIH6JcgTLw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Effective Training Epochs (89 --> 99) (_Run (1 --> 10)ends at 10_ )\n",
        "\n",
        "* lr .001  \n",
        "\n",
        "* Batch size 64\n",
        "\n",
        "* **max val_acc 93.25 in 99th(here 10th) epoch**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "teDRZPckUB6X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/My Drive/2times:ep:81+004-val_acc:0.928.hdf5')\n",
        "#clr_t = CyclicLR(mode='triangular',base_lr=0.000075,max_lr=0.00015,step_size=500)\n",
        "#callbacks_list.append(clr_t)\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=SGD(.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "e9917397-9f58-4c51-8b5c-0d91622e1d5c",
        "id": "c766S167UB7I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=64,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 320s 6ms/step - loss: 0.0503 - acc: 0.9829 - val_loss: 0.2344 - val_acc: 0.9293\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.92960\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 311s 6ms/step - loss: 0.0474 - acc: 0.9835 - val_loss: 0.2321 - val_acc: 0.9285\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.92960\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 311s 6ms/step - loss: 0.0458 - acc: 0.9838 - val_loss: 0.2307 - val_acc: 0.9297\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.92960 to 0.92970, saving model to /content/gdrive/My Drive/test_9.4L/ep:003-val_acc:0.930.hdf5\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 310s 6ms/step - loss: 0.0444 - acc: 0.9849 - val_loss: 0.2287 - val_acc: 0.9301\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.92970 to 0.93010, saving model to /content/gdrive/My Drive/test_9.4L/ep:004-val_acc:0.930.hdf5\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 310s 6ms/step - loss: 0.0442 - acc: 0.9851 - val_loss: 0.2278 - val_acc: 0.9303\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.93010 to 0.93030, saving model to /content/gdrive/My Drive/test_9.4L/ep:005-val_acc:0.930.hdf5\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 310s 6ms/step - loss: 0.0421 - acc: 0.9857 - val_loss: 0.2272 - val_acc: 0.9310\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.93030 to 0.93100, saving model to /content/gdrive/My Drive/test_9.4L/ep:006-val_acc:0.931.hdf5\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 310s 6ms/step - loss: 0.0423 - acc: 0.9852 - val_loss: 0.2265 - val_acc: 0.9309\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.93100\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 310s 6ms/step - loss: 0.0411 - acc: 0.9864 - val_loss: 0.2262 - val_acc: 0.9315\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.93100 to 0.93150, saving model to /content/gdrive/My Drive/test_9.4L/ep:008-val_acc:0.931.hdf5\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 311s 6ms/step - loss: 0.0411 - acc: 0.9868 - val_loss: 0.2259 - val_acc: 0.9319\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.93150 to 0.93190, saving model to /content/gdrive/My Drive/test_9.4L/ep:009-val_acc:0.932.hdf5\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 311s 6ms/step - loss: 0.0407 - acc: 0.9864 - val_loss: 0.2253 - val_acc: 0.9325\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.93190 to 0.93250, saving model to /content/gdrive/My Drive/test_9.4L/ep:010-val_acc:0.932.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa1411ce278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "BUMOBEoR5nUd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Effective Training Epochs (100 --> 104) (_Run (1 -->5)ends at 4_ )\n",
        "\n",
        "* **CLR | 0.001 to 0.01 | Step size:500**\n",
        "\n",
        "* Batch size 64\n",
        "\n",
        "* **max val_acc 93.39 in 104th(here 10th) epoch**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xf90BzkSqDpz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/My Drive/test_9.4L/ep:010-val_acc:0.932.hdf5')\n",
        "clr_t = CyclicLR(mode='triangular',base_lr=0.001,max_lr=0.01,step_size=500)\n",
        "#callbacks_list.append(clr_t)\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=SGD(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "e01b3324-6edb-418a-c1e9-36ad352191f2",
        "id": "Pws6v2h7qDqQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=64,\n",
        "                    epochs=5,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "50000/50000 [==============================] - 326s 7ms/step - loss: 0.0396 - acc: 0.9870 - val_loss: 0.2259 - val_acc: 0.9319\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.93190, saving model to /content/gdrive/My Drive/test_9.4L/ep:001-val_acc:0.932.hdf5\n",
            "Epoch 2/5\n",
            "50000/50000 [==============================] - 311s 6ms/step - loss: 0.0362 - acc: 0.9881 - val_loss: 0.2238 - val_acc: 0.9331\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.93190 to 0.93310, saving model to /content/gdrive/My Drive/test_9.4L/ep:002-val_acc:0.933.hdf5\n",
            "Epoch 3/5\n",
            "50000/50000 [==============================] - 310s 6ms/step - loss: 0.0335 - acc: 0.9891 - val_loss: 0.2246 - val_acc: 0.9324\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.93310\n",
            "Epoch 4/5\n",
            "50000/50000 [==============================] - 310s 6ms/step - loss: 0.0307 - acc: 0.9908 - val_loss: 0.2239 - val_acc: 0.9339\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.93310 to 0.93390, saving model to /content/gdrive/My Drive/test_9.4L/ep:004-val_acc:0.934.hdf5\n",
            "Epoch 5/5\n",
            "50000/50000 [==============================] - 310s 6ms/step - loss: 0.0289 - acc: 0.9910 - val_loss: 0.2259 - val_acc: 0.9339\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.93390\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0571e30198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "Cd3mXGb86DKX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Effective Training Epochs (105 --> 109) (_Run (1 --> 5)ends at 4_ )\n",
        "\n",
        "* **CLR | 0.0001 to 0.001 | Step Size : 500**\n",
        "\n",
        "* Batch size 64\n",
        "\n",
        "* **max val_acc 93.41 in 109th(here 4th) epoch**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mGnqGLZYytCq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath=\"/content/gdrive/My Drive/test_9.4L/ep:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "#model.load_weights('/content/gdrive/My Drive/test_9.4L/ep:010-val_acc:0.932.hdf5')\n",
        "clr_t = CyclicLR(mode='triangular',base_lr=0.0001,max_lr=0.001,step_size=500)\n",
        "callbacks_list.append(clr_t)\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=SGD(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "7fb6b17c-6020-426e-e794-03a3ffd275cc",
        "id": "NARKuyOIytDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=64,\n",
        "                    epochs=5,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                   callbacks=callbacks_list)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "  128/50000 [..............................] - ETA: 45:33 - loss: 0.0124 - acc: 1.0000  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.361691). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 320s 6ms/step - loss: 0.0262 - acc: 0.9928 - val_loss: 0.2246 - val_acc: 0.9337\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.93370, saving model to /content/gdrive/My Drive/test_9.4L/ep:001-val_acc:0.934.hdf5\n",
            "Epoch 2/5\n",
            "50000/50000 [==============================] - 314s 6ms/step - loss: 0.0259 - acc: 0.9932 - val_loss: 0.2243 - val_acc: 0.9338\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.93370 to 0.93380, saving model to /content/gdrive/My Drive/test_9.4L/ep:002-val_acc:0.934.hdf5\n",
            "Epoch 3/5\n",
            "50000/50000 [==============================] - 313s 6ms/step - loss: 0.0272 - acc: 0.9927 - val_loss: 0.2250 - val_acc: 0.9334\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.93380\n",
            "Epoch 4/5\n",
            "50000/50000 [==============================] - 312s 6ms/step - loss: 0.0263 - acc: 0.9929 - val_loss: 0.2245 - val_acc: 0.9341\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.93380 to 0.93410, saving model to /content/gdrive/My Drive/test_9.4L/ep:004-val_acc:0.934.hdf5\n",
            "Epoch 5/5\n",
            "50000/50000 [==============================] - 313s 6ms/step - loss: 0.0258 - acc: 0.9930 - val_loss: 0.2244 - val_acc: 0.9331\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.93410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f057320d9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    }
  ]
}